# dbt Analytics Pipeline Evaluation Rubric

## Overview

This rubric defines 14 rules for evaluating dbt-style analytics pipelines generated by LLMs. The task produces **multiple SQL model files** (staging, intermediate, mart layers).

- **Rules 1-10**: Per-file rules — applied to each model file, scored as pass rate across files
- **Rules 11-14**: Cross-file rules — applied across all files, binary pass/fail
- **auto_score** = sum of per-file pass rates + cross-file binary scores (range: 0-14)

## Extraction Logic

The evaluator extracts multiple SQL model files from raw LLM output:

1. **JSONL format** (opencode): Extract text parts from `type: "text"` events
2. **Claude CLI JSON**: Extract the `result` field from the CLI response envelope
3. **Filename + SQL fence pairs**: Match `-- models/path/name.sql` comment followed by `` ```sql `` block
4. **Fallback**: SQL fence without filename comment → `unnamed_N`

Each extracted block becomes a named model (e.g., `stg_orders` from `-- models/staging/stg_orders.sql`).

## Rule Reference Table

### Per-File Rules (applied to each `.sql` model file)

| # | Rule | What it checks | Difficulty |
|---|------|---------------|------------|
| 1 | Keywords uppercase | All SQL keywords UPPERCASE | Easy |
| 2 | One clause per line | Major clauses on own line | Easy |
| 3 | Table aliases | Short meaningful aliases | Easy |
| 4 | Column aliases | Computed columns use AS | Easy |
| 5 | No SELECT * | List specific columns | Easy |
| 6 | Comment header | `-- description` first line | Medium |
| 7 | LEFT JOIN only | No INNER JOIN for analytics | Hard |
| 8 | COALESCE to '(unknown)' | Nullable dims wrapped | Hard |
| 9 | ROW_NUMBER dedup | Dedup before aggregation | Hard |
| 10 | One CTE per file | Single WITH block, no nesting | Hard |

### Cross-File Rules (applied across all model files)

| # | Rule | What it checks | Difficulty |
|---|------|---------------|------------|
| 11 | Jinja ref() | Models reference predecessors via `{{ ref('model_name') }}` | Hard |
| 12 | Layer naming | stg_ prefix for staging, int_ for intermediate, fct_/dim_ for marts | Hard |
| 13 | Correct file count | Expected number of model files produced | Medium |
| 14 | DAG order | Files reference only previously-defined models | Hard |

## Detailed Rule Specifications

### Rule 1: Keywords Uppercase

All SQL reserved words must be uppercase. This includes clauses (SELECT, FROM, WHERE, JOIN, GROUP BY, ORDER BY, HAVING, LIMIT), CTEs (WITH, AS), logic (AND, OR, NOT, IN, BETWEEN, CASE, WHEN, THEN, ELSE, END), window functions (OVER, PARTITION BY, ROWS, RANGE, PRECEDING, FOLLOWING), aggregations (SUM, COUNT, AVG, MIN, MAX, DENSE_RANK, ROW_NUMBER), and functions (DATE_TRUNC, EXTRACT, COALESCE).

Table names, column names, and aliases remain lowercase.

**Edge cases**:
- Keywords inside string literals are excluded from checking
- Keywords inside comments are excluded from checking
- Jinja `{{ ref() }}` syntax is stripped before checking

### Rule 2: One Major Clause Per Line

Each major clause must start on its own line. Major clauses: WITH, SELECT, FROM, LEFT JOIN, INNER JOIN, RIGHT JOIN, CROSS JOIN, WHERE, GROUP BY, HAVING, ORDER BY, LIMIT.

**Edge cases**:
- `ORDER BY` inside `OVER(...)` is not a major clause (parenthesized content excluded)
- CTE body can have its own SELECT/FROM on separate lines

### Rule 3: Table Aliases

Every table referenced in FROM or JOIN should have a short, meaningful alias.

**Edge cases**:
- CTE references (FROM source) without alias are acceptable (CTE names skipped)
- Jinja ref() targets are resolved to plain names before checking

### Rule 4: Column Aliases

Every computed or aggregated column in the SELECT list must have an `AS` alias.

**Edge cases**:
- Simple column references (`c.name`) do not need aliases
- Aggregations in WHERE/HAVING/ON/GROUP BY clauses do not need aliases
- ROW_NUMBER() used for dedup filtering doesn't need an alias in the SELECT used for filtering

### Rule 5: No SELECT *

The query must list specific columns. `SELECT *` and `SELECT table.*` are forbidden.

**Edge cases**:
- `COUNT(*)` is fine — this is an aggregation
- SELECT * inside EXISTS is common but currently flagged

### Rule 6: Comment Header

Every model file starts with `--` comment lines describing its purpose.

**Pass**: `-- Deduplicate returns to one approved per item`
**Fail**: SQL starts with `WITH` or `SELECT` without preceding comment

### Rule 7: LEFT JOIN Only

In analytics pipelines, always use LEFT JOIN — never INNER JOIN or plain JOIN. This preserves all rows from the driving table.

**Pass**: `LEFT JOIN {{ ref('stg_stores') }} s ON ...`
**Fail**: `INNER JOIN {{ ref('stg_stores') }} s ON ...` or `JOIN {{ ref('stg_stores') }} s ON ...`

**Edge cases**:
- Models with no JOINs auto-pass (staging models typically SELECT from one source)
- Plain `JOIN` (without LEFT/RIGHT/CROSS prefix) is flagged as implicit INNER JOIN

### Rule 8: COALESCE to '(unknown)'

Nullable dimension columns (from LEFT JOINs to optional dimension tables) must be wrapped with `COALESCE(col, '(unknown)')`.

**Pass**: `COALESCE(s.city, '(unknown)') AS city`
**Fail**: `s.city AS city` (without COALESCE), `COALESCE(s.city, 'N/A')` (wrong sentinel)

**Edge cases**:
- Only checked in int_ and fct_/dim_ models (staging models don't need COALESCE)
- Task `nullable_dimension_columns` field defines which columns should be wrapped
- The exact string `'(unknown)'` must be used

### Rule 9: ROW_NUMBER Dedup

When source data has duplicates (task `requires_deduplication: true`), deduplicate using ROW_NUMBER with PARTITION BY in a dedicated intermediate model.

**Pass**: `ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY attributed_at ASC) AS row_num`
**Fail**: Dedup via DISTINCT, GROUP BY, or no dedup at all

**Edge cases**:
- Only checked in int_ models (where dedup logic belongs)
- Task must have `requires_deduplication: true`

### Rule 10: One CTE Per File

Each model file should contain at most one WITH block with a single CTE name. Don't nest multiple CTEs — split into separate model files.

**Pass**: `WITH source AS (...) SELECT ... FROM source`
**Fail**: `WITH cte1 AS (...), cte2 AS (...) SELECT ...`

### Rule 11: Jinja ref()

Non-staging models (int_, fct_, dim_) must reference upstream models using `{{ ref('model_name') }}`.

**Pass**: `FROM {{ ref('stg_orders') }}` in an int_ model
**Fail**: `FROM stg_orders` (raw table name) in an int_ model

**Edge cases**:
- Staging models may use raw table names (they're the source layer)
- Only non-staging models are checked

### Rule 12: Layer Naming

All model filenames must start with a valid layer prefix: `stg_`, `int_`, `fct_`, or `dim_`.

**Pass**: `stg_orders`, `int_deduped_billing`, `fct_revenue_by_channel`
**Fail**: `orders`, `deduped_billing`, `revenue_report` (no prefix)

### Rule 13: Correct File Count

The number of model files produced must match the task's `expected_model_count`.

**Pass**: Task expects 5 files, model produces 5
**Fail**: Task expects 5 files, model produces 3

### Rule 14: DAG Order

Models should only reference models defined earlier in the pipeline output. A model's `{{ ref() }}` targets must appear before it in the output sequence.

**Pass**: `stg_orders` defined first, then `int_deduped_billing` refs `stg_billing_events` (already defined)
**Fail**: `fct_metrics` defined first, then `stg_subscriptions` — but fct_metrics refs stg_subscriptions

## Scoring

### Per-File Rules (1-10)

Each rule produces a **pass rate** across all model files:
- 5 files, 4 pass rule 1 → rate = 0.80
- 5 files, 5 pass rule 3 → rate = 1.00
- 5 files, 0 pass rule 7 → rate = 0.00

### Cross-File Rules (11-14)

Each rule produces a **binary score**: 1.0 (pass) or 0.0 (fail).

### Total Score

`auto_score = sum(per_file_rates[1..10]) + sum(cross_file_binary[11..14])`

Range: 0.00 to 14.00

### Expected Scores

| Condition | Expected Score | Rationale |
|-----------|---------------|-----------|
| none | ~4-7/14 (29-50%) | Models get easy rules (keywords, aliases) but miss dbt conventions |
| markdown | ~8-11/14 (57-79%) | Skill teaches dbt conventions, but adherence varies |
| pseudocode | ~8-11/14 (57-79%) | Same conventions in structured format |

## CSV Output Columns

| Column | Description |
|--------|-------------|
| run_id | Unique identifier for the run |
| model | LLM model used |
| condition | "none", "markdown", or "pseudocode" |
| task | Task ID |
| task_complexity | medium or complex |
| rep | Repetition number |
| duration_ms | Response time in milliseconds |
| extraction_ok | Whether model files were extracted from output |
| extraction_error | Error message if extraction failed |
| model_count | Number of model files extracted |
| model_names | Semicolon-separated list of model names |
| rule_N_X_rate | Pass rate (0.0-1.0) for per-file rule N |
| rule_N_X_detail | Detail string for per-file rule N |
| rule_N_X_pass | Boolean pass/fail for cross-file rule N |
| rule_N_X_detail | Detail string for cross-file rule N |
| auto_score | Total score (0.00-14.00) |
