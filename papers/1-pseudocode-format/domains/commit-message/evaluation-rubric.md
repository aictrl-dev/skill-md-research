# Commit Message Evaluation Rubric

## Overview

This rubric defines 14 rules for evaluating team-convention commit messages generated by LLMs. All 14 rules are fully automated — no manual review required.

The rule set combines 5 universal Conventional Commits rules (~100% baseline) with 9 opinionated team-specific rules (~0-40% baseline each). Expected baseline without skill: 40-60%. Expected with skill: 85-93%.

## Extraction Logic

The evaluator extracts commit messages from raw LLM output using this priority:

1. **JSONL format** (opencode): Extract text parts from `type: "text"` events
2. **Claude CLI JSON**: Extract the `result` field from the CLI response envelope
3. **Permission denials**: Check for Write tool permission denials (Haiku fallback)
4. **Code fences**: Look for content inside `` ```text ``, `` ```commit ``, `` ```git ``, or plain `` ``` `` blocks
5. **Header pattern**: Find text after "commit message:", "here's the commit:", etc.
6. **Direct match**: Scan lines for one starting with a valid type prefix (feat, fix, etc.)

Trailing LLM explanation is trimmed using heuristics that detect meta-commentary vs. legitimate commit body/footer content.

## Rule Reference Table

| # | Category | Rule | Baseline Est. | Pass Example | Fail Example |
|---|----------|------|---------------|-------------|-------------|
| 1 | UNIVERSAL | Valid type prefix | ~100% | `feat(api): ...` | `feature(api): ...` |
| 2 | UNIVERSAL | Correct separator `": "` | ~100% | `fix: handle null` | `fix:handle null` |
| 3 | UNIVERSAL | Imperative mood | ~100% | `add rate limiting` | `added rate limiting` |
| 4 | UNIVERSAL | No trailing period | ~100% | `fix null pointer` | `fix null pointer.` |
| 5 | UNIVERSAL | Lowercase start | ~100% | `handle null record` | `Handle null record` |
| 6 | TEAM | Scope from vocabulary | ~20% | `fix(db): ...` (db in allowed list) | `fix(user-service): ...` (not in list) |
| 7 | TEAM | Gitmoji after type | ~0% | `fix(db): :bug: handle null` | `fix(db): handle null` |
| 8 | TEAM | Body "Why/What" sections | ~5% | Body has `Why:` and `What:` headers | Prose body without sections |
| 9 | TEAM | Body word count 30-150 | ~40% | 45-word body | 10-word body or no body |
| 10 | TEAM | Trailer key-value format | ~70% | `Ticket: PROJ-247` | `Refs #247` (no colon) |
| 11 | TEAM | Signed-off-by footer | ~5% | `Signed-off-by: Alice <a@e.com>` | Missing footer |
| 12 | TEAM | BREAKING CHANGE footer | ~60% | `BREAKING CHANGE: tokens invalidated...` | Missing when task.breaking=true |
| 13 | TEAM | JIRA-style ticket ref | ~5% | `Ticket: PROJ-247` | `Refs: #247` or missing |
| 14 | TEAM | Subject ≤ 50 chars | ~30% | `fix(db): :bug: handle null` (30 chars) | 65-char subject line |

## Detailed Rule Specifications

### Rule 1: Valid Type Prefix

**Valid types**: feat, fix, docs, style, refactor, perf, test, build, ci, chore, revert

- `feature` is invalid (must be `feat`)
- Type must be all lowercase
- Type must be the first token on the line

### Rule 2: Correct Separator

Exactly `: ` (colon followed by single space). No extra spaces, tabs, or missing space.

### Rule 3: Imperative Mood

First word of description (after gitmoji if present) must not be past tense, gerund, or third person. Checked against blacklist of ~50 words.

### Rule 4: No Trailing Period

Description must not end with `.`

### Rule 5: Lowercase Description Start

First alphabetic character of description must be lowercase. The gitmoji shortcode (`:bug:`, etc.) starts with `:` which passes. The check looks at the first character of the description text.

### Rule 6: Scope from Vocabulary

Scope must be present AND be one of the values in `task.allowed_scopes`. If the task defines `allowed_scopes: ["auth", "api", "db", "ui", "core"]`, then only these 5 scopes are valid.

**Why models fail (~20%)**: Models pick semantically reasonable scopes like "user-service" or "users" that aren't in the allowed vocabulary.

### Rule 7: Gitmoji After Type

The description must start with the correct gitmoji shortcode for the commit type, followed by a space. The mapping is defined in `task.gitmoji_map`.

Example: For `fix` type with `gitmoji_map.fix = ":bug:"`, description must start with `:bug: `.

**Why models fail (~0%)**: Models never add gitmoji shortcodes unprompted.

### Rule 8: Body "Why/What" Sections

Body must contain both `Why:` and `What:` section headers (case-insensitive line-start match).

**Why models fail (~5%)**: Models write prose paragraphs, not structured sections with explicit headers.

### Rule 9: Body Word Count

Body text must be between `task.body_min_words` (default 30) and `task.body_max_words` (default 150) words. Word count excludes footer lines.

**Why models fail (~40%)**: Models either write too little (1-line body) or too much (200+ words).

### Rule 10: Trailer Key-Value Format

All footers must use the git-trailer `Token: value` format (token, colon, space, value). This is verified by checking that every parsed footer has a non-empty token and value.

**Why models mostly pass (~70%)**: Most models naturally use colon-separated footers.

### Rule 11: Signed-off-by Footer

Must have a `Signed-off-by` footer matching `task.signed_off_by` exactly.

**Why models fail (~5%)**: The DCO sign-off convention is not standard. Models never add it without instruction.

### Rule 12: BREAKING CHANGE Footer

Only checked when `task.breaking_change == true`. Must have `BREAKING CHANGE:` footer with value >= 10 characters.

**Why models partially pass (~60%)**: Models aware of breaking changes sometimes add this footer, but not always.

### Rule 13: JIRA-Style Ticket Reference

Must have a `Ticket` footer with value `{jira_project}-{jira_number}` (e.g., `Ticket: PROJ-247`). GitHub-style `#247` or `Refs: #247` does NOT count.

**Why models fail (~5%)**: Models default to GitHub-style `#123` references, never JIRA format.

### Rule 14: Subject Length ≤ 50 Characters

Full subject line (type + scope + separator + description) must be 50 characters or fewer. This is stricter than the common 72-char convention.

**Why models fail (~30%)**: Most models optimize for the 72-char limit, producing subjects of 50-70 chars.

## Expected Outputs Per Task

### Task 1: Simple Bugfix

```
fix(db): :bug: handle null record in findById

Why:
UserService.findById() throws a null pointer exception when
querying for a user ID that does not exist in the database.
This crashes the request handler silently.

What:
Add null check after database query and return a proper 404
response with descriptive error message instead of crashing.

Ticket: PROJ-247
Signed-off-by: Alice Developer <alice@example.com>
```

**Score**: 14/14

### Task 2: Medium Feature

```
feat(api): :sparkles: add rate limiting

Why:
The API has no protection against abuse. A single client can
make unlimited requests, potentially causing service
degradation for all users.

What:
Implement sliding window rate limiter backed by Redis. Default
config allows 100 requests per 15-minute window. Returns 429
with Retry-After header when the limit is exceeded.

Ticket: PROJ-312
Signed-off-by: Alice Developer <alice@example.com>
```

**Score**: 14/14

### Task 3: Breaking Change

```
feat(auth)!: :sparkles: replace jwt lib

Why:
The jsonwebtoken library has known CVEs and lacks native
TypeScript types. jose is standards-compliant and provides
full TS support with zero native dependencies.

What:
Migrate all token signing from jwt.sign() to jose SignJWT
builder pattern. Verification now uses jwtVerify() with
explicit algorithm allowlist.

BREAKING CHANGE: JWT tokens signed with jsonwebtoken are
incompatible with jose. All active sessions will be
invalidated. Users must log in again after deployment.

Ticket: PROJ-189
Signed-off-by: Alice Developer <alice@example.com>
```

**Score**: 14/14

## Scoring

- **auto_score**: Count of rules that pass (0-14). All 14 are fully automated.
- **scored_rules**: Always 14.
- **needs_manual_review**: Always False.
- **Expected baseline (no skill)**: 6-8/14 (43-57%). The 5 universal rules pass plus ~1-3 team rules partially pass.
- **Expected with skill**: 12-14/14 (86-100%).

## CSV Output Columns

| Column | Description |
|--------|-------------|
| run_id | Unique identifier for the run |
| model | LLM model used |
| condition | "none", "markdown", or "pseudocode" |
| task | Task ID |
| task_complexity | simple, medium, complex |
| rep | Repetition number |
| duration_ms | Response time in milliseconds |
| extraction_ok | Whether commit message was extracted |
| extraction_error | Error message if extraction failed |
| structure_valid | Basic structural validity |
| structure_errors | List of structural errors |
| rule_N_X_pass | Boolean pass/fail for rule N |
| rule_N_X_detail | Detail string for rule N |
| auto_score | Total rules passed (0-14) |
| scored_rules | Number of scored rules (always 14) |
