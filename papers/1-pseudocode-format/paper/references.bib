% ─── Pseudocode Prompting ────────────────────────────────────────────────────

@inproceedings{mishra2023prompting,
  title     = {Prompting with Pseudo-Code Instructions},
  author    = {Mishra, Mayank and Kumar, Prince and Bhat, Riyaz and
               Murthy, Rudra and Contractor, Danish and Tamilselvam, Srikanth},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in
               Natural Language Processing (EMNLP)},
  pages     = {15178--15197},
  year      = {2023},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.emnlp-main.939/},
}

@article{kumar2025training,
  title   = {Training with Pseudo-Code for Instruction Following},
  author  = {Kumar, Prince and Murthy, Rudra and Bhat, Riyaz and
             Contractor, Danish},
  journal = {arXiv preprint arXiv:2505.18011},
  year    = {2025},
}

% ─── Prompt Format Effects ───────────────────────────────────────────────────

@article{he2024prompt,
  title   = {Does Prompt Formatting Have Any Impact on {LLM} Performance?},
  author  = {He, Jia and Rungta, Mukund and Koleczek, David and
             Sekhon, Arshdeep and Wang, Franklin X and Hasan, Sadid},
  journal = {arXiv preprint arXiv:2411.10541},
  year    = {2024},
}

@inproceedings{sclar2024quantifying,
  title     = {Quantifying Language Models' Sensitivity to Spurious Features
               in Prompt Design},
  author    = {Sclar, Melanie and Choi, Yejin and Tsvetkov, Yulia and
               Suhr, Alane},
  booktitle = {Proceedings of the Twelfth International Conference on
               Learning Representations (ICLR)},
  year      = {2024},
}

@inproceedings{errica2025sensitivity,
  title     = {What Did {I} Do Wrong? {Q}uantifying {LLMs}' Sensitivity and
               Consistency to Prompt Engineering},
  author    = {Errica, Federico and Sanvito, Davide and Siracusano, Giuseppe
               and Bifulco, Roberto},
  booktitle = {Proceedings of the 2025 Conference of the North American
               Chapter of the Association for Computational Linguistics
               (NAACL)},
  year      = {2025},
  url       = {https://aclanthology.org/2025.naacl-long.73/},
}

@inproceedings{ngweta2025mof,
  title     = {Towards {LLMs} Robustness to Changes in Prompt Format Styles},
  author    = {Ngweta, Lilian and Kate, Kiran and Tsay, Jason and Rizk, Yara},
  booktitle = {Proceedings of the 2025 Conference of the North American
               Chapter of the Association for Computational Linguistics:
               Student Research Workshop},
  year      = {2025},
  url       = {https://aclanthology.org/2025.naacl-srw.51/},
}

@article{liu2025cfpo,
  title   = {Beyond Prompt Content: Enhancing {LLM} Performance via
             Content-Format Integrated Prompt Optimization},
  author  = {Liu, Yuanye and others},
  journal = {arXiv preprint arXiv:2502.04295},
  year    = {2025},
}

% ─── Agent Skills & Benchmarks ───────────────────────────────────────────────

@article{li2026skillsbench,
  title   = {{SkillsBench}: Benchmarking How Well Agent Skills Work Across
             Diverse Tasks},
  author  = {Li, Xiangyi and others},
  journal = {arXiv preprint arXiv:2602.12670},
  year    = {2026},
}

@article{xu2026agentskills,
  title   = {Agent Skills for Large Language Models},
  author  = {Xu, Renjun and Yan, Yang},
  journal = {arXiv preprint arXiv:2602.12430},
  year    = {2026},
}

% ─── Code-as-Prompt & Reasoning ──────────────────────────────────────────────

@inproceedings{chae2024compilers,
  title     = {Language Models as Compilers: Simulating Pseudocode Execution
               Improves Algorithmic Reasoning in Language Models},
  author    = {Chae, Hyungjoo and others},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in
               Natural Language Processing (EMNLP)},
  pages     = {22471--22502},
  year      = {2024},
}

@inproceedings{madaan2022language,
  title     = {Language Models of Code are Few-Shot Commonsense Learners},
  author    = {Madaan, Aman and Zhou, Shuyan and Alon, Uri and Yang, Yiming
               and Neubig, Graham},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in
               Natural Language Processing (EMNLP)},
  year      = {2022},
}

% ─── Surveys ─────────────────────────────────────────────────────────────────

@article{plaat2025agentic,
  title   = {Agentic Large Language Models, a Survey},
  author  = {Plaat, Aske and others},
  journal = {arXiv preprint arXiv:2503.23037},
  year    = {2025},
}

% ─── Agent Frameworks ────────────────────────────────────────────────────────

@article{singh2023progprompt,
  title   = {{ProgPrompt}: Program Generation for Situated Robot Task Planning
             using Large Language Models},
  author  = {Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and
             Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and
             Fox, Dieter and Thomason, Jesse and Garg, Animesh},
  journal = {Autonomous Robots},
  volume  = {47},
  pages   = {999--1012},
  year    = {2023},
}

% ─── Statistical Methods ─────────────────────────────────────────────────────

@article{cliff1993dominance,
  title   = {Dominance Statistics: Ordinal Analyses to Answer Ordinal Questions},
  author  = {Cliff, Norman},
  journal = {Psychological Bulletin},
  volume  = {114},
  number  = {3},
  pages   = {494--509},
  year    = {1993},
}

@book{efron1994bootstrap,
  title     = {An Introduction to the Bootstrap},
  author    = {Efron, Bradley and Tibshirani, Robert J.},
  year      = {1994},
  publisher = {Chapman and Hall/CRC},
}

% ─── Constrained Decoding & Structural Priors ──────────────────────────────

@article{willard2023efficient,
  title   = {Efficient Guided Generation for Large Language Models},
  author  = {Willard, Brandon T and Louf, R{\'e}mi},
  journal = {arXiv preprint arXiv:2307.09702},
  year    = {2023},
}

% ─── Domain Specific: SQL & DevOps ──────────────────────────────────────────

@article{liu2024survey,
  title   = {A Survey of {Text-to-SQL} in the Era of {LLMs}: {W}here are we,
             and where are we going?},
  author  = {Liu, Xinyu and others},
  journal = {arXiv preprint arXiv:2408.05109},
  year    = {2024},
}

@inproceedings{lyu2026automatic,
  title     = {Automatic {D}ockerfile Generation with Large Language Models},
  author    = {Lyu, Jun and Zhang, He and Yuan, Yusong and Yang, Lanxin and
               Li, Yue and Rigger, Manuel},
  booktitle = {Proceedings of the 48th International Conference on Software
               Engineering (ICSE)},
  year      = {2026},
}

% ─── Self-Correction ────────────────────────────────────────────────────────

@article{madaan2023selfrefine,
  title   = {{Self-Refine}: Iterative Refinement with Self-Feedback},
  author  = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and
             Hallinan, Skyler and Gao, Luyu and others},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  year    = {2023},
}
