---
name: run-experiment
description: Run a multi-model, multi-condition LLM experiment for a domain with resumable execution, JSON result capture, and automatic evaluator scoring.
---

# Run LLM Experiment

Run a controlled experiment comparing LLM outputs across models and conditions for a given domain.

## Experiment Structure

Every experiment follows the same grid:

```
Models × Conditions × Tasks × Reps = Total Runs
```

- **Models**: `haiku`, `opus` (Claude CLI), `zai-coding-plan/glm-4.7`, `zai-coding-plan/glm-5` (OpenCode CLI)
- **Conditions**: `none` (no skill), `markdown` (prose skill), `pseudocode` (code skill)
- **Tasks**: Domain-specific task JSON files in `domains/{domain}/test-data/`
- **Reps**: 5 per cell (default)

## Directory Layout

```
domains/{domain}/
  skills/{name}-markdown/SKILL.md    # Prose skill file
  skills/{name}-pseudocode/SKILL.md  # Pseudocode skill file
  test-data/task-{n}-{name}.json     # Task definitions
  results/                           # Output directory
    {model}_{condition}_task{n}_rep{r}.json   # Individual run results
    scores.csv                       # Evaluated scores (generated by evaluator)
    raw_outputs.zip                  # Archived JSON outputs
  evaluate_{domain}.py               # Domain-specific evaluator
```

## Run Script Pattern

Each run must:

1. **Skip existing** — check if `{results_dir}/{run_id}.json` exists before running
2. **Strip YAML frontmatter** from SKILL.md files before injecting into prompts
3. **Prefix skill content** with `Follow these {domain_label} guidelines:` then `---` separator before task prompt
4. **Capture full JSON output** — use `--output-format json` for Claude, `--format json` for OpenCode
5. **Record metadata** — run_id, model, condition, task, complexity, rep, timestamp, duration_ms, cli_tool, raw_output
6. **Sleep between runs** — 3-5 seconds to avoid rate limiting
7. **Use nohup** for long experiments (>20 runs) to avoid shell timeouts

### CLI invocation

```bash
# Claude models
cd /tmp && cat "$prompt_file" | claude -p - --model "$MODEL" \
  --output-format json --no-session-persistence \
  --mcp-config "$EMPTY_MCP" 2>&1

# OpenCode/GLM models
opencode run -m "$model_id" --format json -f "$prompt_file" -- "Follow the instructions." 2>&1
```

Always `unset CLAUDECODE` before running to prevent nesting errors.
Always create an empty MCP config: `echo '{"mcpServers":{}}' > /tmp/experiment-empty-mcp.json`

### Frontmatter stripping

```bash
strip_frontmatter() {
  echo "$1" | awk 'BEGIN{skip=0; fm=0} /^---$/{if(fm==0){fm=1;skip=1;next}else if(fm==1){fm=2;next}} fm==2||fm==0{print}'
}
```

## Run ID Convention

```
{safe_model}_{condition}_task{task_id}_rep{rep}
```

Where `safe_model` replaces `/` with `-` (e.g., `zai-coding-plan-glm-5`).

## After Running

1. Run the domain evaluator: `PYTHONPATH=scripts python3 domains/{domain}/evaluate_{domain}.py domains/{domain}/results/*_rep[1-5].json`
2. Check extraction rate: should be >95% (2-3 failures per 180 runs is normal for GLM models)
3. Quick sanity check: `python3 -c "import pandas as pd; df=pd.read_csv('domains/{domain}/results/scores.csv'); print(df.groupby('condition')['auto_score'].mean())"`
4. Zip results: `cd domains/{domain}/results && zip -q raw_outputs.zip *.json`

## Resumability

The script is fully resumable — rerun the same script and it skips completed runs. This is critical because:
- Long experiments (180+ runs) take 30-60 minutes
- API rate limits or transient failures may interrupt execution
- Background processes may be killed by shell timeouts
